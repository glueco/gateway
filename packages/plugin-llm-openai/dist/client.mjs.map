{"version":3,"sources":["../src/client.ts"],"sourcesContent":["// ============================================\n// OPENAI PLUGIN CLIENT\n// Typed client wrapper for target apps\n// ============================================\n//\n// This module provides typed client wrappers for the OpenAI LLM plugin.\n// It depends only on the SDK transport interface and shared contracts.\n//\n// Usage:\n// ```ts\n// import { openai } from \"@glueco/plugin-llm-openai/client\";\n// import { GatewayClient } from \"@glueco/sdk\";\n//\n// const client = new GatewayClient({ ... });\n// const transport = await client.getTransport();\n// const openaiClient = openai(transport);\n//\n// const response = await openaiClient.chatCompletions({\n//   model: \"gpt-4o\",\n//   messages: [{ role: \"user\", content: \"Hello!\" }]\n// });\n// ```\n// ============================================\n\nimport type {\n  GatewayTransport,\n  GatewayResponse,\n  GatewayStreamResponse,\n  GatewayRequestOptions,\n} from \"@glueco/sdk\";\n\nimport {\n  type ChatCompletionRequest,\n  type ChatCompletionResponse,\n  PLUGIN_ID,\n} from \"./contracts\";\n\n// Re-export contracts for consumer convenience\nexport * from \"./contracts\";\n\n// ============================================\n// CLIENT TYPES\n// ============================================\n\n/**\n * Options for chat completion requests.\n */\nexport interface ChatCompletionOptions extends Omit<\n  GatewayRequestOptions,\n  \"stream\" | \"method\"\n> {\n  /**\n   * Override for custom behavior (advanced usage).\n   */\n  raw?: boolean;\n}\n\n/**\n * OpenAI client interface.\n * Provides typed methods for all supported actions.\n */\nexport interface OpenAIClient {\n  /**\n   * Create a chat completion.\n   *\n   * @param request - Chat completion request (OpenAI format)\n   * @param options - Optional request options\n   * @returns Chat completion response\n   *\n   * @example\n   * ```ts\n   * const response = await openaiClient.chatCompletions({\n   *   model: \"gpt-4o\",\n   *   messages: [\n   *     { role: \"system\", content: \"You are a helpful assistant.\" },\n   *     { role: \"user\", content: \"What is the capital of France?\" }\n   *   ],\n   *   temperature: 0.7,\n   *   max_tokens: 1000\n   * });\n   *\n   * console.log(response.data.choices[0].message.content);\n   * ```\n   */\n  chatCompletions(\n    request: ChatCompletionRequest,\n    options?: ChatCompletionOptions,\n  ): Promise<GatewayResponse<ChatCompletionResponse>>;\n\n  /**\n   * Create a streaming chat completion.\n   *\n   * @param request - Chat completion request (stream flag will be set automatically)\n   * @param options - Optional request options\n   * @returns Streaming response with SSE stream\n   *\n   * @example\n   * ```ts\n   * const response = await openaiClient.chatCompletionsStream({\n   *   model: \"gpt-4o\",\n   *   messages: [{ role: \"user\", content: \"Tell me a story\" }]\n   * });\n   *\n   * const reader = response.stream.getReader();\n   * const decoder = new TextDecoder();\n   *\n   * while (true) {\n   *   const { done, value } = await reader.read();\n   *   if (done) break;\n   *   const chunk = decoder.decode(value);\n   *   // Process SSE chunk\n   * }\n   * ```\n   */\n  chatCompletionsStream(\n    request: Omit<ChatCompletionRequest, \"stream\">,\n    options?: ChatCompletionOptions,\n  ): Promise<GatewayStreamResponse>;\n\n  /**\n   * Get the underlying transport for advanced usage.\n   * Useful when you need direct access to the gateway.\n   */\n  readonly transport: GatewayTransport;\n}\n\n// ============================================\n// CLIENT FACTORY\n// ============================================\n\n/**\n * Create a typed OpenAI client.\n *\n * @param transport - Gateway transport from SDK\n * @returns Typed OpenAI client\n *\n * @example\n * ```ts\n * import { openai } from \"@glueco/plugin-llm-openai/client\";\n * import { GatewayClient } from \"@glueco/sdk\";\n *\n * // Setup\n * const gatewayClient = new GatewayClient({\n *   keyStorage: new FileKeyStorage('./.gateway/keys.json'),\n *   configStorage: new FileConfigStorage('./.gateway/config.json'),\n * });\n *\n * // Get transport and create typed client\n * const transport = await gatewayClient.getTransport();\n * const openaiClient = openai(transport);\n *\n * // Use with full type safety\n * const response = await openaiClient.chatCompletions({\n *   model: \"gpt-4o\",\n *   messages: [{ role: \"user\", content: \"Hello!\" }]\n * });\n * ```\n */\nexport function openai(transport: GatewayTransport): OpenAIClient {\n  return {\n    transport,\n\n    async chatCompletions(\n      request: ChatCompletionRequest,\n      options?: ChatCompletionOptions,\n    ): Promise<GatewayResponse<ChatCompletionResponse>> {\n      // Ensure stream is false for non-streaming requests\n      const payload = { ...request, stream: false };\n\n      return transport.request<ChatCompletionResponse, ChatCompletionRequest>(\n        PLUGIN_ID,\n        \"chat.completions\",\n        payload,\n        options,\n      );\n    },\n\n    async chatCompletionsStream(\n      request: Omit<ChatCompletionRequest, \"stream\">,\n      options?: ChatCompletionOptions,\n    ): Promise<GatewayStreamResponse> {\n      return transport.requestStream(\n        PLUGIN_ID,\n        \"chat.completions\",\n        request,\n        options,\n      );\n    },\n  };\n}\n\n// Default export for convenient importing\nexport default openai;\n"],"mappings":";;;;;;;;;;;;;;;;;;;AA8JO,SAAS,OAAO,WAA2C;AAChE,SAAO;AAAA,IACL;AAAA,IAEA,MAAM,gBACJ,SACA,SACkD;AAElD,YAAM,UAAU,EAAE,GAAG,SAAS,QAAQ,MAAM;AAE5C,aAAO,UAAU;AAAA,QACf;AAAA,QACA;AAAA,QACA;AAAA,QACA;AAAA,MACF;AAAA,IACF;AAAA,IAEA,MAAM,sBACJ,SACA,SACgC;AAChC,aAAO,UAAU;AAAA,QACf;AAAA,QACA;AAAA,QACA;AAAA,QACA;AAAA,MACF;AAAA,IACF;AAAA,EACF;AACF;AAGA,IAAO,iBAAQ;","names":[]}